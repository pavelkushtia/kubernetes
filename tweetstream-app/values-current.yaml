# Current Working Configuration for TweetStream
# This matches the actual deployed and working setup

api:
  image:
    repository: 192.168.1.82:5555/tweetstream/api
    tag: "1.0.0"
    pullPolicy: Always
  replicaCount: 3
  resources:
    limits:
      cpu: 150m
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 128Mi

frontend:
  enabled: true
  image:
    repository: 192.168.1.82:5555/tweetstream/frontend
    tag: "2.3.0"
    pullPolicy: Always
  replicaCount: 2
  service:
    targetPort: 8080  # Our frontend runs on port 8080
  resources:
    limits:
      cpu: 100m
      memory: 64Mi
    requests:
      cpu: 50m
      memory: 32Mi
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 10
    periodSeconds: 10
  readinessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 5

database:
  persistence:
    enabled: true
    size: 10Gi
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi

redis:
  persistence:
    enabled: true
    size: 2Gi
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/rewrite-target: ""
  hosts:
  - host: tweetstream.192.168.1.82.nip.io
    paths:
    - path: /
      pathType: Prefix
      service:
        name: tweetstream-frontend
        port: 80
    - path: /api
      pathType: Prefix
      service:
        name: tweetstream-api
        port: 3000

kafka:
  enabled: true
  name: kafka
  image:
    repository: confluentinc/cp-kafka
    tag: "7.4.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 9092
    controllerPort: 9093
  persistence:
    enabled: true
    storageClass: "local-path"
    size: 5Gi
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 512Mi

monitoring:
  enabled: true

# Security context that works with our setup
security:
  podSecurityContext: {}
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL

# Allow scheduling on any node
nodeSelector: {}

tolerations: [] 